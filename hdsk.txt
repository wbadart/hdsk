-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Unified data science toolkit for Haskell and more
--   
--   <h1>Haskell Data Science Kit</h1>
--   
--   
--   The Haskell Data Science Kit (<i>HDSK</i>) project is an attempt to
--   create a well-documented, well-tested, and performant data science
--   library implemented in the Haskell language.
--   
--   Sources suggest that in spite of huge potential for performance gains
--   over current de facto methods [1], adoption of Haskell in the data
--   science community lags for a variety of reasons, the greatest of which
--   seems to be the dearth [2] of easy-to-use data science libraries
--   (indeed, searching for "<i>data science</i>" on GitHub yields
--   <b>14</b> Haskell-language repositories and <b>5,807</b>
--   Python-language repositories [3]). This project seeks to mediate that
--   issue by presenting a <i>unified</i> (though modular) library of data
--   science utilities which support the entire life-cycle of a data
--   science project.
--   
--   <b>Disclaimer:</b> At the time of writing, I am still a beginner in
--   Haskell, and this project is as much about the above stated goal as it
--   is about me learning and practicing Haskell itself and the software
--   development ecosystem around it. So, I make no guarantees that I will
--   give the most optimal or idiomatic solution to any given function (and
--   in cases when I don't, pull requests are gladly welcomed!).
--   
--   <ol>
--   
--   <li><a>https://izbicki.me/blog/hlearn-cross-validates-400x-faster-than-weka</a></li>
--   
--   <li><a>https://www.linkedin.com/pulse/haskell-data-science-good-bad-ugly-tom-hutchins</a></li>
--   
--   <li><a>https://github.com/search?q=data+science+language%3AHaskell&amp;type=Repositories</a></li>
--   </ol>
--   
--   <h2>Installation</h2>
--   
--   To use HDSK within your <b>stack</b> project, you must add this
--   repository to the <tt>extra-deps</tt> list in <tt>stack.yaml</tt>.
--   NOTE: this step will change once HDSK is released on Hackage.
--   
--   <pre>
--   extra-deps:
--   - git: git@github.com:wbadart/hdsk.git
--   commit: a52bed4216f607628e71594256dafd550ffe2d3e
--   </pre>
--   
--   The commit hash listed above is the most recent commit at the time of
--   this writing. Be sure that the value you use is a recent enough to
--   contain the features you need.
--   
--   The cabal file generated by stack has been checked in, so if you
--   aren't using stack, and are only using cabal, the library can be
--   installed from a fresh clone of the repository.
--   
--   <h2>Usage</h2>
--   
--   Please see <a>wbadart.info/hdsk</a> for library documentation. Further
--   project info, such as planned features, is made available on the wiki:
--   <a>https://github.com/wbadart/hdsk/wiki</a>.
--   
--   <h2>License</h2>
--   
--   You'll notice a key theme in this document has been <b>promoting
--   adoption.</b> As such, I'm developing and eventually releasing this
--   project under the BSD-3-Clause [1] license, due to its general
--   permissiveness. This is also one of the more popular licenses among
--   the Haskell community [2].
--   
--   Please see <a>LICENSE</a> for the full text.
--   
--   <ol>
--   <li><a>https://opensource.org/licenses/BSD-3-Clause</a></li>
--   
--   <li><a>https://wiki.haskell.org/How_to_write_a_Haskell_program</a></li>
--   </ol>
@package hdsk
@version 0.1.0.0


-- | This module contains basic numerical descriptive statistics that are
--   applicable to a vector of doubles.
module Hdsk.Description

-- | <i>O(n)</i> Computes the arithmetic mean of a collection of numbers.
mean :: (Foldable f, Fractional n) => f n -> n

-- | <i>O(n)</i> Computes the unbiased variance of a collection of numbers.
var :: (Foldable f, Functor f, Floating n) => f n -> n

-- | <i>O(n)</i> Computes the standard deviation of a collection of
--   numbers.
std :: (Foldable f, Functor f, Floating n) => f n -> n

-- | <i>O(n)</i> Selects the element which is greater than <tt>p</tt>% of
--   the rest. When the <tt>p</tt>-th percentile does not land directly on
--   a whole index, midpoint interpolation is used to average left and
--   right side of the split.
percentile :: (Selectable p, RealFrac n) => n -> p n -> n

-- | <i>O(n)</i> Finds the median element the collection.
median :: (Selectable p, RealFrac n) => p n -> n

-- | <i>O(n)</i> Finds the first quartile of a collection of numbers.
q1 :: (Selectable p, RealFrac n) => p n -> n

-- | <i>O(n)</i> Finds the third quartile of a collection of numbers.
q3 :: (Selectable p, RealFrac n) => p n -> n

-- | <i>O(n)</i> Inter-quartile range. The distance between the first and
--   third quartiles.
iqr :: (Selectable p, RealFrac n) => p n -> n

-- | <i>O(n)</i> Simple implementation of quickselct (aka Hoare's algorithm
--   or k-rank). Selects the <tt>k</tt>-smallest element from the
--   collection.
select :: (Selectable p, Ord a) => Int -> p a -> a

-- | Defines a container which is suitable for the k-rank/ select
--   algorithm.
class Foldable p => Selectable p
instance Hdsk.Description.Selectable Data.Vector.Vector
instance Hdsk.Description.Selectable Data.Sequence.Internal.Seq
instance Hdsk.Description.Selectable []


module Hdsk.Bins

-- | <i>O(n)</i> Map each number in a vector of continuous data to a bin
--   number according to its percentile (makes even-width bins). Bins are
--   0-indexed.
bin :: (Functor f, Selectable f) => [Double -> Bool] -> f Double -> f Int

-- | <i>O(kn)</i> Generate the bounds of fixed width bins, encoded as
--   predicates of sample quantiles.
genIntervals :: Selectable f => Int -> f Double -> [Double -> Bool]


-- | A standard set of metrics for evaluating both classification and
--   regression models. In Big-O notation, let <i>C</i> represent the
--   number of classes and <i>n</i> the number of predictions. In general,
--   we assume <i>n &gt;&gt; C</i>, so that <i>n</i> terms dominate
--   <i>C</i> terms.
module Hdsk.Metrics

-- | <i>O(n)</i> Given the ground truth <tt>yTrue</tt> and predictions
--   <tt>yPred</tt>, the expression <tt>accuracy c yTrue yPred</tt> reports
--   the accuracy of the predictions, <i>(TP + TN) / (P + N)</i>. Here,
--   <tt>c</tt> is the list of class labels, e.g. <tt>["cat", "dog"]</tt>.
--   Accuracy represents the proportion of classifications which were
--   correct.
accuracy :: (Zippable z, Eq a, Fractional b) => [a] -> z a -> z a -> b

-- | <i>O(C^2)</i> where <i>C</i> is the number of classes. Calculate
--   accuracy from a confusion matrix, rather than a list of truths and
--   predictions.
accuracyCM :: Fractional a => Matrix Int -> a

-- | <i>O(n)</i> Compute the precision (positive predictive value) of a
--   list of predictions, given the class list, target class label, and
--   ground truths. The below example show how to find precision for a
--   binary classifier.
--   
--   <pre>
--   &gt;&gt;&gt; truth = ["cat", "cat", "not cat"]
--   
--   &gt;&gt;&gt; preds = ["cat", "not cat", "not cat"]
--   
--   &gt;&gt;&gt; precision ["cat", "not cat"] "cat" truth preds
--   0.5
--   </pre>
--   
--   The expression <i>TP / (TP + FP)</i> represents precision in terms of
--   counts of true and false predictions. It is the proportion of positive
--   predictions which are true.
--   
--   Precision is undefined when no positive predictions are made.
precision :: (Zippable z, Eq a, Fractional b) => [a] -> a -> z a -> z a -> b

-- | <i>O(C^2)</i> Compute the precision directly from a confusion matrix.
--   The second argument is the class index within the confusion matrix.
--   For instance, given the class list
--   
--   <pre>
--   &gt;&gt;&gt; ["dog", "cat"]
--   </pre>
--   
--   The index of class <i>dog</i> is <tt>1</tt> and the index of class
--   <i>cat</i> is <tt>2</tt> (since the matrix is 1-indexed).
precisionCM :: Fractional a => Matrix Int -> Int -> a

-- | <i>O(n)</i> Compute the recall (sensitivity, true positive/ hit rate)
--   of a list of predictions, given ground truth, class list, and target
--   class label. See <tt>precision</tt> for discussion of arguments.
--   
--   Recall, in terms of the confusion matrix, is <i>TP / (TP + FN)</i>,
--   and represents the proportion of positive predictions which were
--   classified as such.
--   
--   Recall is undefined when there are no positive observations.
recall :: (Zippable z, Eq a, Fractional b) => [a] -> a -> z a -> z a -> b

-- | <i>O(C^2)</i> Compute recall from a confusion matrix for a specified
--   class. See <tt>precisionCM</tt> for discussion on the class index
--   argument.
recallCM :: Fractional a => Matrix Int -> Int -> a

-- | <i>O(n)</i> Compute the specificity (true negative rate) of the
--   predictions given a list of class labels, a target class, and ground
--   truth. <i>TN / (FP + TN)</i>, the proportion of negative objects
--   correctly labeled.
--   
--   Specificity is undefined when there are no negative truths.
specificity :: (Zippable z, Eq a, Fractional b) => [a] -> a -> z a -> z a -> b

-- | <i>O(C^2)</i> Compute specificity from a confusion matrix for a
--   specified class. See <tt>precisionCM</tt> for discussion on the class
--   index argument.
specificityCM :: Fractional a => Matrix Int -> Int -> a

-- | <i>O(n)</i> Compute the balanced f1-score of the model for a given
--   class.
f1 :: (Zippable z, Eq a, Fractional b) => [a] -> a -> z a -> z a -> b

-- | <i>O(C^2)</i> Compute the f1-score from a confusion matrix. See
--   <tt>precisionCM</tt> for a discussion of the class index argument.
f1CM :: Fractional a => Matrix Int -> Int -> a

-- | <i>O(C^2 + n)</i> where <i>C</i> is the number of classes and <i>n</i>
--   the number of predictions. In general, <i>n &gt;&gt; C</i>. Generates
--   the confusion matrix for the predictions of an N-class predictor. The
--   result will be a 1-indexed NxN matrix where rows represent the
--   predicted class and columns the actual class. Classes are encoded as
--   indices, where the index of a class within the matrix corresponds to
--   its index within the set of classes.
confusionMatrix :: (Zippable z, Eq a) => [a] -> z a -> z a -> Matrix Int

-- | <i>O(1)</i> Count the true positives for a class in a given confusion
--   matrix.
tp :: Num a => Matrix Int -> Int -> a

-- | <i>O(C)</i> Count the false positives for a class in a given confusion
--   matrix.
fp :: Num a => Matrix Int -> Int -> a

-- | <i>O(C^2)</i> Count the true negatives for a class in a given
--   confusion matrix.
tn :: Num a => Matrix Int -> Int -> a

-- | <i>O(C)</i> Count the false negatives for a class in a given confusion
--   matrix.
fn :: Num a => Matrix Int -> Int -> a

-- | <i>O(n)</i> Find the mean squared error of the regression. The squared
--   error of an estimation is the square of its difference with the
--   corresponding observation. The first argument is the list of
--   observations, and the second is the list of corresponding estimations.
meanSqError :: Floating a => [a] -> [a] -> a

-- | <i>O(n)</i> Find the mean absolute error of the regression. The
--   absolute error of an estimate is the absolute value of its difference
--   with the corresponding observation.
meanAbsError :: Fractional a => [a] -> [a] -> a

-- | <i>O(n)</i> Find the explained variance of a regression. Explained
--   variance measures the proportion of the observations is accounted for
--   by the regression.
explainedVariance :: (Eq a, Floating a) => [a] -> [a] -> a

-- | <i>O(n)</i> Find the coefficient of determination (R^2 value) of a
--   regression. Aka goodness of fit.
r2score :: (Eq a, Floating a) => [a] -> [a] -> a

-- | Class of types which support the zip and zipWith operations.
class Foldable z => Zippable z
instance Hdsk.Metrics.Zippable Data.Vector.Vector
instance Hdsk.Metrics.Zippable []


-- | A simple implementation of a Naive Bayes classifier for purely
--   categorical data.
module Hdsk.NaiveBayes

-- | Type alias for the conditional probability table data structure. Maps
--   labels to lookup tables where lookup tables map from feature values to
--   the number of times that feature value was observed with the parent
--   label.
type CPT = Map String (Map String Int)

-- | /O(n * k * (log l + log m)) where n is the number of data points, k is
--   the length of each data point, l is the number of labels, and m is the
--   gretest number of feature values for any feature./ Generate the
--   conditional probability tables over the dataset. Counts the occurences
--   of each feature value given a class. Maps from class label to a map
--   from feature value to count.
mkTables :: [[String]] -> [CPT]

-- | /O(l k m) where l is the number of labels, k is the length of each
--   feature vector, and m is the number of feature values./ Given a list
--   of conditional probability tables and an unlabeled tuple, predicts the
--   label of the tuple.
classify :: [CPT] -> [String] -> String

-- | /O(k m) where k is the length of the feature vector and m is the
--   number of feature values./ Given conditional probability tables, a
--   tuple, and a label calculate the posterior probability of the label
--   given that tuple. P(C|X)
posterior :: [CPT] -> [String] -> String -> Double

-- | <i>O(m)</i> Given a class, feature value, and conditional probability
--   table, compute the likelihood of that configuration. P(x_i|c)
likelihood :: String -> String -> CPT -> Double

-- | <i>O(m) where m is the number of feature values.</i> Computes the
--   frequency of the given label from the given conditional prob. table.
labelCount :: Num n => CPT -> String -> n

-- | <i>O(l m)</i> Calculate the number of training instances from a CPT.
dataLength :: Num n => CPT -> n


-- | This module contains functions which support efficient numerical
--   methods, e.g. approximating derivatives.
module Hdsk.Numerical

-- | Consume a stream of sequentially improving estimations until some
--   measure of error fails to improve by <i>eta</i>. Requires at least two
--   estimates to be produced by the stream. Assumes that error decreases
--   with each estimate.
terminate :: Double -> (a -> Double) -> [a] -> a


-- | This module provides an implementation of the k-means clustering
--   algorithm. K-means is well suited to problems where the number of
--   clusters is known and fixed (it is a parameter to the algorithm).
module Hdsk.Cluster

-- | <i>O(inkD)</i> Run the kmeans clustering algorithm over the given
--   dataset. Returns a list of cluster labels which corresponds 1-1 with
--   the input list of data points.
kmeans :: Int -> [[Double]] -> [Int]

-- | <i>O(ikD*n^2)</i> Run the kmedoids clustering algorithm over the given
--   dataset.
kmedoids :: Int -> [[Double]] -> [Int]

-- | Convenience function for creating clustering function. For instance,
--   <a>kmeans</a>, as defined in this module, is a <a>kclusterer</a> with
--   <a>distEuclidean</a> distance metric, <a>centroid</a> center measure,
--   and <i>eta = 0.01</i>.
kclusterer :: Eq tup => Double -> (tup -> tup -> Double) -> ([tup] -> tup) -> Int -> [tup] -> [Int]

-- | <i>O(nkD)</i> where <i>n</i> is the number of data points, <i>k</i> is
--   the number of clusters, and <i>D</i> is the dimensionality of the
--   data. Run one iteration of the k-means algorithm. The parameter
--   <i>k</i>, number of clusters, is implied by the length of the list of
--   initial centroids. Returns a list of cluster labels (encoded as
--   integers) which correspond 1-1 with the given list of data points.
cluster :: (Eq tup, Ord d) => (tup -> tup -> d) -> [tup] -> [tup] -> [Int]

-- | <i>O(inkD)</i> where <i>n</i> is the number of data points, <i>k</i>
--   is the number of clusters, <i>D</i> is the dimensionality of the data,
--   and <i>i</i> is the number of iterations. Improve the quality of the
--   clustering. Generates an infinite list of clusterings.
improve :: (Eq tup, Ord d) => (tup -> tup -> d) -> ([tup] -> tup) -> [tup] -> [Int] -> [[Int]]

-- | <i>O(nD)</i> Calculate the mean squared distance from each point in a
--   cluster to the centroid.
meanSqDist :: (Ord d, Floating d) => (tup -> tup -> d) -> ([tup] -> tup) -> [tup] -> [Int] -> d

-- | <i>O(nD)</i> where <i>n</i> is the number of data points and <i>D</i>
--   is the dimensionality of the data. Calculate the midpoints of clusters
--   according to a metric (<a>centroid</a>, for instance).
midpoints :: ([tup] -> tup) -> [Int] -> [tup] -> [tup]

-- | <i>O(nD)</i> where <i>n</i> is the number of points and <i>D</i> is
--   the dimensionality of each point. Compute the mean of a list of
--   D-dimensional points.
centroid :: Fractional a => [[a]] -> [a]

-- | <i>O(n^2)</i> Compute the medoid of a list of points.
medoid :: (Ord d, Fractional d) => (tup -> tup -> d) -> [tup] -> tup

-- | <i>O(kD)</i> where <i>k</i> is the number of points to consider and
--   <i>D</i> is the dimensionality of the data. Select from a list of
--   points that which is closest to a given point <i>x</i> according to a
--   given distance metric. For instance, in one dimension:
--   
--   <pre>
--   &gt;&gt;&gt; closestTo (\x y -&gt; abs $ sum (zipWith (-) x y)) [0] [[1], [2]]
--   
--   &gt;&gt;&gt; [1.0]
--   </pre>
closestTo :: Ord d => (tup -> tup -> d) -> tup -> [tup] -> tup

-- | <i>O(D)</i> where <i>D</i> is the dimensionality of the vectors.
--   General distance metric between two vectors/ data points in the same
--   vector space. The first parameter <tt>p</tt> corresponds to the
--   <i>L^p</i> space to compute the norm (e.g. <tt>p = 2</tt> is Euclidean
--   distance).
minkowski :: Floating a => a -> [a] -> [a] -> a

-- | Common case of <a>minkowski</a> is with <i>p = 1</i> for Manhattan
--   distance.
distManhattan :: Floating a => [a] -> [a] -> a

-- | Common case of <a>minkowski</a> is with <i>p = 2</i> for Euclidean
--   distance.
distEuclidean :: Floating a => [a] -> [a] -> a


-- | This module contains several general utility functions used throughout
--   the library. Exporting this because they may be helpful to users too.
module Hdsk.Util

-- | <i>O(n log n)</i> Compute the most frequent label in the data set.
--   Ties do not have well defined behavior; the tie is broken by
--   <a>maximumBy</a> which doesn't have clear documentation on ties.
majorityLabel :: (Eq label, Ord label) => (tup -> label) -> [tup] -> label

-- | <i>O(n log n)</i> Given a container, construct a map from the unique
--   elements of the container to their frequency within the container.
count :: (Foldable f, Ord k) => f k -> Map k Int

-- | <i>O(n)</i> Given a container and a predicate, calculate the number of
--   elements which pass the predicate
countBy :: Foldable f => (a -> Bool) -> f a -> Int

-- | <i>O(n)</i> Version of <a>countBy</a> with generic result.
countBy' :: (Foldable f, Num b) => (a -> Bool) -> f a -> b

-- | <i>O(n)</i> Given a container, construct a hash table from the unique
--   elements of the container to their frequency within the container. I
--   recommend this function over <a>count</a> when 1) counting performance
--   is critical, AND 2) you're familiar with the ST monad.
countHT :: (Foldable f, Eq k, Hashable k) => f k -> ST s (HashTable s k Int)

-- | <i>O(n)</i> Convenience function for getting the count of a single
--   element from a container.
countElemHT :: (Foldable f, Eq k, Hashable k) => f k -> k -> Int

-- | <i>O(n)</i> Shorthand for <tt>genericLength</tt> which works on all
--   foldables.
length' :: (Num a, Foldable f) => f b -> a

-- | <i>O(1)</i> Shorthand for <tt>logBase 2</tt>
lg :: Floating a => a -> a


-- | This module implements the K Nearest Neighbors classifier, which finds
--   the k tuples "closest" to the unobserved instance and combines their
--   labels via some voting/ consensus metric to produce a prediction.
module Hdsk.NearestNeighbors

-- | Assemble a KNN classifier.
knnclassifier :: Ord d => ([tup] -> label) -> (tup -> tup -> d) -> Int -> [tup] -> tup -> label

-- | <i>O(nD log k)</i> where <i>D</i> is the cost of the distance
--   function. Select the <tt>k</tt> nearest training instances to the
--   argument tuple, according to the given distance metric (greater value
--   means more dissimilar).
knn :: Ord d => (tup -> tup -> d) -> Int -> [tup] -> tup -> [tup]

-- | <i>O(n log n)</i> Compute the most frequent label in the data set.
--   Ties do not have well defined behavior; the tie is broken by
--   <a>maximumBy</a> which doesn't have clear documentation on ties.
majorityLabel :: (Eq label, Ord label) => (tup -> label) -> [tup] -> label


-- | This module implements various measures of entropy and information
--   gain in service of decision tree classification.
module Hdsk.DecisionTree.Entropy

-- | <i>O(n log n)</i> Calculate the total information entropy of a
--   dataset.
entropy :: (Functor f, Foldable f, Eq label, Ord label, Floating a) => (tup -> label) -> f tup -> a

-- | <i>O(nC)</i> Compute the conditional entropy of splitting the dataset
--   over the given branches.
conditionalEntropy :: Eq label => (tup -> label) -> [tup] -> [tup -> Bool] -> Double


-- | This module facilitates the definition of decision tree classifiers
--   with tools for combining and parametrizing entropy measures, pruning
--   tactics, and more.
module Hdsk.DecisionTree

-- | <i>O(???)</i> Predict the label of an unobserved tuple, according to
--   the provided parameters.
classify :: (Eq label, Ord label) => ([tup] -> [[tup -> Bool]]) -> (tup -> label) -> ((tup -> label) -> [tup] -> [tup -> Bool] -> Double) -> [tup] -> tup -> label

-- | <i>O(nC + n log n)</i> Compute the information gain of the given
--   branching. Branches are encoded as a list of predicates over data
--   instances. An object belongs to whichever branch for which it passes
--   the predicate. Assumes that each data object will pass one and only
--   one of the branching predicates.
infoGain :: (Eq label, Ord label) => (tup -> label) -> [tup] -> [tup -> Bool] -> Double

-- | <i>O(nB)</i> where <i>B</i> is the number of branches. Compute the
--   split info of the branching.
splitInfo :: (Eq label, Ord label) => (tup -> label) -> [tup] -> [tup -> Bool] -> Double

-- | <i>O(nB + nC + n log n)</i> Compute the gain ratio of a branching.
gainRatio :: (Eq label, Ord label) => (tup -> label) -> [tup] -> [tup -> Bool] -> Double

-- | <i>O(n log n)</i> Compute the branching predicates for the feature
--   values at the given index. The feature values at index <tt>idx</tt>
--   should be categorical.
mkCatTests :: Ord a => (tup -> a) -> [tup] -> [tup -> Bool]
