-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Unified data science toolkit for Haskell and more
--   
--   Please see the README on GitHub at
--   <a>https://github.com/wbadart/hdsk</a>
@package hdsk
@version 0.1.0.0


module Hdsk.Bayesian.Bins


-- | A simple implementation of a Naive Bayes classifier for purely
--   categorical data.
module Hdsk.Bayesian.NaiveBayes

-- | Type alias for the conditional probability table data structure. Maps
--   labels to lookup tables where lookup tables map from feature values to
--   the number of times that feature value was observed with the parent
--   label.
type CPT = Map String (Map String Int)

-- | /O(n * k * (log l + log m)) where n is the number of data points, k is
--   the length of each data point, l is the number of labels, and m is the
--   gretest number of feature values for any feature./ Generate the
--   conditional probability tables over the dataset. Counts the occurences
--   of each feature value given a class. Maps from class label to a map
--   from feature value to count.
mkTables :: [[String]] -> [CPT]

-- | /O(l k m) where l is the number of labels, k is the length of each
--   feature vector, and m is the number of feature values./ Given a list
--   of conditional probability tables and an unlabeled tuple, predicts the
--   label of the tuple.
classify :: [CPT] -> [String] -> String

-- | /O(k m) where k is the length of the feature vector and m is the
--   number of feature values./ Given conditional probability tables, a
--   tuple, and a label calculate the posterior probability of the label
--   given that tuple. P(C|X)
posterior :: [CPT] -> [String] -> String -> Double

-- | <i>O(m)</i> Given a class, feature value, and conditional probability
--   table, compute the likelihood of that configuration. P(x_i|c)
likelihood :: String -> String -> CPT -> Double

-- | <i>O(m) where m is the number of feature values.</i> Computes the
--   frequency of the given label from the given conditional prob. table.
labelCount :: Num n => CPT -> String -> n

-- | <i>O(l m)</i> Calculate the number of training instances from a CPT.
dataLength :: Num n => CPT -> n


-- | This module contains basic numerical descriptive statistics that are
--   applicable to a vector of doubles.
module Hdsk.Description

-- | <i>O(n)</i> Computes the arithmetic mean of a collection of numbers.
mean :: (Foldable f, Fractional n) => f n -> n

-- | <i>O(n)</i> Computes the unbiased variance of a collection of numbers.
var :: (Foldable f, Functor f, Floating n) => f n -> n

-- | <i>O(n)</i> Computes the standard deviation of a collection of
--   numbers.
std :: (Foldable f, Functor f, Floating n) => f n -> n

-- | <i>O(n)</i> Selects the element which is greater than <tt>p</tt>% of
--   the rest. When the <tt>p</tt>-th percentile does not land directly on
--   a whole index, midpoint interpolation is used to average left and
--   right side of the split.
percentile :: (Selectable p, RealFrac n) => n -> p n -> n

-- | <i>O(n)</i> Finds the median element the collection.
median :: (Selectable p, RealFrac n) => p n -> n

-- | <i>O(n)</i> Finds the first quartile of a collection of numbers.
q1 :: (Selectable p, RealFrac n) => p n -> n

-- | <i>O(n)</i> Finds the third quartile of a collection of numbers.
q3 :: (Selectable p, RealFrac n) => p n -> n

-- | <i>O(n)</i> Inter-quartile range. The distance between the first and
--   third quartiles.
iqr :: (Selectable p, RealFrac n) => p n -> n

-- | <i>O(n)</i> Simple implementation of quickselct (aka Hoare's algorithm
--   or k-rank). Selects the <tt>k</tt>-smallest element from the
--   collection.
select :: (Selectable p, Ord a) => Int -> p a -> a

-- | Defines a container which is suitable for the k-rank/ select
--   algorithm.
class Foldable p => Selectable p
instance Hdsk.Description.Selectable Data.Vector.Vector
instance Hdsk.Description.Selectable Data.Sequence.Internal.Seq
instance Hdsk.Description.Selectable []


-- | A standard set of metrics for evaluating both classification and
--   regression models. In Big-O notation, let <i>C</i> represent the
--   number of classes and <i>n</i> the number of predictions. In general,
--   we assume <i>n &gt;&gt; C</i>, so that <i>n</i> terms dominate
--   <i>C</i> terms.
module Hdsk.Metrics

-- | <i>O(n)</i> Given the ground truth <tt>yTrue</tt> and predictions
--   <tt>yPred</tt>, the expression <tt>accuracy c yTrue yPred</tt> reports
--   the accuracy of the predictions, <i>(TP + TN) / (P + N)</i>. Here,
--   <tt>c</tt> is the set of class labels, e.g. <tt>S.fromList ["cat",
--   "dog"]</tt>. The proportion of classifications which were correct.
accuracy :: (Eq a, Fractional b) => [a] -> [a] -> [a] -> b

-- | <i>O(C^2)</i> where <i>C</i> is the number of classes. Calculate
--   accuracy from a confusion matrix, rather than a list of truths and
--   predictions.
accuracyCM :: Fractional a => Matrix Int -> a

-- | <i>O(n)</i> Compute the precision (positive predictive value) of a
--   list of predictions, given the class list, target class label, and
--   ground truths. The below example show how to find precision for a
--   binary classifier.
--   
--   <pre>
--   &gt;&gt;&gt; truth = ["cat", "cat", "not cat"]
--   
--   &gt;&gt;&gt; preds = ["cat", "not cat", "not cat"]
--   
--   &gt;&gt;&gt; precision ["cat", "not cat"] "cat" truth preds
--   0.5
--   </pre>
--   
--   The expression <i>TP / (TP + FP)</i> represents precision in terms of
--   counts of true and false predictions. It is the proportion of positive
--   predictions which are true.
--   
--   Precision is undefined when no positive predictions are made.
precision :: (Eq a, Fractional b) => [a] -> a -> [a] -> [a] -> b

-- | <i>O(C^2)</i> Compute the precision directly from a confusion matrix.
--   The second argument is the class index within the confusion matrix.
--   For instance, given the class list
--   
--   <pre>
--   &gt;&gt;&gt; ["dog", "cat"]
--   </pre>
--   
--   The index of class <i>dog</i> is <tt>1</tt> and the index of class
--   <i>cat</i> is <tt>2</tt> (since the matrix is 1-indexed).
precisionCM :: Fractional a => Matrix Int -> Int -> a

-- | <i>O(n)</i> Compute the recall (sensitivity, true positive/ hit rate)
--   of a list of predictions, given ground truth, class list, and target
--   class label. See <tt>precision</tt> for discussion of arguments.
--   
--   Recall, in terms of the confusion matrix, is <i>TP / (TP + FN)</i>,
--   and represents the proportion of positive predictions which were
--   classified as such.
--   
--   Recall is undefined when there are no positive observations.
recall :: (Eq a, Fractional b) => [a] -> a -> [a] -> [a] -> b

-- | <i>O(C^2)</i> Compute recall from a confusion matrix for a specified
--   class. See <tt>precisionCM</tt> for discussion on the class index
--   argument.
recallCM :: Fractional a => Matrix Int -> Int -> a

-- | <i>O(n)</i> Compute the specificity (true negative rate) of the
--   predictions given a list of class labels, a target class, and ground
--   truth. <i>TN / (FP + TN)</i>, the proportion of negative objects
--   correctly labeled.
--   
--   Specificity is undefined when there are no negative truths.
specificity :: (Eq a, Fractional b) => [a] -> a -> [a] -> [a] -> b

-- | <i>O(C^2)</i> Compute specificity from a confusion matrix for a
--   specified class. See <tt>precisionCM</tt> for discussion on the class
--   index argument.
specificityCM :: Fractional a => Matrix Int -> Int -> a

-- | <i>O(n)</i> Compute the balanced f1-score of the model for a given
--   class.
f1 :: (Eq a, Fractional b) => [a] -> a -> [a] -> [a] -> b

-- | <i>O(C^2)</i> Compute the f1-score from a confusion matrix. See
--   <tt>precisionCM</tt> for a discussion of the class index argument.
f1CM :: Fractional a => Matrix Int -> Int -> a

-- | <i>O(C^2 + n)</i> where <i>C</i> is the number of classes and <i>n</i>
--   the number of predictions. In general, <i>n &gt;&gt; C</i>.
--   
--   Generates the confusion matrix for the predictions of an N-class
--   predictor. The result will be a 1-indexed NxN matrix where rows
--   represent the predicted class and columns the actual class. Classes
--   are encoded as indices, where the index of a class within the matrix
--   corresponds to its index within the set of classes.
confusionMatrix :: Eq a => [a] -> [a] -> [a] -> Matrix Int

-- | <i>O(1)</i> Count the true positives for a class in a given confusion
--   matrix.
tp :: Num a => Matrix Int -> Int -> a

-- | <i>O(C)</i> Count the false positives for a class in a given confusion
--   matrix.
fp :: Num a => Matrix Int -> Int -> a

-- | <i>O(C^2)</i> Count the true negatives for a class in a given
--   confusion matrix.
tn :: Num a => Matrix Int -> Int -> a

-- | <i>O(C)</i> Count the false negatives for a class in a given confusion
--   matrix.
fn :: Num a => Matrix Int -> Int -> a

-- | <i>O(n)</i> Find the mean squared error of the regression. The squared
--   error of an estimation is the square of its difference with the
--   corresponding observation. The first argument is the list of
--   observations, and the second is the list of corresponding estimations.
meanSqError :: Floating a => [a] -> [a] -> a

-- | <i>O(n)</i> Find the mean absolute error of the regression. The
--   absolute error of an estimate is the absolute value of its difference
--   with the corresponding observation.
meanAbsError :: Fractional a => [a] -> [a] -> a

-- | <i>O(n)</i> Find the explained variance of a regression. Explained
--   variance measures the proportion of the observations is accounted for
--   by the regression.
explainedVariance :: (Eq a, Floating a) => [a] -> [a] -> a

-- | <i>O(n)</i> Find the coefficient of determination (R^2 value) of a
--   regression. Aka goodness of fit.
r2score :: (Eq a, Floating a) => [a] -> [a] -> a
